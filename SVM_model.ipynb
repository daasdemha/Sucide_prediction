{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3fe8957",
   "metadata": {},
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860646ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run X_and_y_selection.ipynb  #  importing a .ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f10c4",
   "metadata": {},
   "source": [
    "# Function to evaluate different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier_function(model, X_train, y_train,X_test,y_test, title):  # function takes the name of the \n",
    "                                                               # model used, the x and y traning and testing sets.\n",
    "  model.fit(X_train, y_train)  # Building the k-nearest neighbors classification model.\n",
    "\n",
    "  y_test_p = model.predict(X_test)  # Predicted results.\n",
    "  print(\"  results\\npred-Actual\")  # printing predicted and real values.\n",
    "  print(np.concatenate((y_test_p.reshape(len(y_test_p),1),y_test.reshape(len(y_test),1)),1))  # Predicted results and \n",
    "                                                                                       #  real results in a np array.\n",
    "   \n",
    "\n",
    "  train_accuracy = round(model.score(X_train,y_train),2) * 100  # Getting traing accuracy multipling it by 100 after \n",
    "                                                                # rounding it by 2 to get a score between 0 to 100\n",
    "  test_accuracy = round(model.score(X_test,y_test),2) * 100  # Getting testing accuracy multipling it by 100 after \n",
    "                                                                # rounding it by 2 to get a score between 0 to 100\n",
    "\n",
    "  print(\"Model train accuracy: \", train_accuracy, \"%\")  # printing the model accurcy. \n",
    "  print(\"Model test accuracy: \", test_accuracy, \"%\")  # printing the model accurcy. \n",
    "\n",
    "\n",
    "  print(\"\\n\\n\")  # printing a new line.\n",
    "  # getting Accuracy or recall or precision or specificity\n",
    "  y_test_pred = model.predict(X_test)  # predicted results\n",
    "  \n",
    "  cReport = classification_report(y_test,y_test_pred)  # creating a Classification report\n",
    "  print(cReport)  # creating a Classification report\n",
    "  \n",
    "  cm = confusion_matrix(y_test, y_test_p)  # creating the confusion matrix\n",
    "  cm2 = multilabel_confusion_matrix(y_test, y_test_pred)  # creating a mutable confusion matrix\n",
    "\n",
    "\n",
    "  precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_test_pred)  # getting the precision,\n",
    "                                                                                          # recall and f1score for later use.\n",
    "  accuracy  = round(np.trace(cm) / float(np.sum(cm)), 2) * 100  # getting aaccuracy and multipling it by 100 after \n",
    "                                                                # rounding it by 2 to get a score between 0 to 100.\n",
    "  precision = round(np.mean(precision),2) * 100  # multipling precision variables mean by 100 after rounding it by\n",
    "                                                #  2 to get a score between 0 to 100.\n",
    "  recall = round(np.mean(recall),2) * 100  # multipling recall variables mean by 100 after rounding it by 2 to \n",
    "                                           # get a score between 0 to 100.\n",
    "  f1_score = round(np.mean(f1_score),2) * 100  # multipling f1_score variables mean by 100 after rounding \n",
    "                                               # it by 2 to get a score between 0 to 100.\n",
    "\n",
    "  lable_list = []  # creating a empty list\n",
    "\n",
    "  for i in range(len(cm)):  # looping in the range of the length of the confusion matrix.\n",
    "    for j in range(len(cm)):  # looping in the range of the length of the confusion matrix.\n",
    "        if j == i:  # if the value of j is equal to the value of i.\n",
    "            # the below code appends the Actual Values Classified correctly to the variable lable_list.\n",
    "            lable_list.append(\"Actual \"+ str(i) +\"\\n\" + \"calssified as \"+ str(j) +\"\\n\" + str(cm[i][j]) + \"\\n\"+ \\\n",
    "                              str(round(cm[i][j]/np.sum(cm),2)) + \" %\")\n",
    "\n",
    "        else:   # otherwise\n",
    "            #  the below function appends the actual values classified wrongly to the variable lable list.\n",
    "            lable_list.append(\"Actual \"+ str(i) +\"\\n\" + \"calssified as \"+ str(j) + \"\\n\"  + str(cm[i][j]) + \"\\n\"+ \\\n",
    "                              str(round(cm[i][j]/np.sum(cm),2)) + \" %\")\n",
    "\n",
    "            \n",
    "  lable_list = np.asarray(lable_list).reshape(len(cm),len(cm))  # resahping the label list as a numpy array to be \n",
    "                                                                # used in plotting the confusion matrix\n",
    "  \n",
    "  #  the variable function will be will be used to display the results of the evaluation to the confusion matrix.\n",
    "  total_score = (\"Accuracy:   \" + str(accuracy) +\" %\" + \"\\nPrecison:    \"  + str(precision)  +\" %\" + \"\\nRecall:        \" +\\\n",
    "                 str(recall)  +\" %\" + \"\\nF1 score:    \"  + str(f1_score) +\" %\") \n",
    "\n",
    "\n",
    "  # Below is the code used to plot the confusion matrix.\n",
    "  plt.figure(figsize = (12,9))  # sets the size of the matrix\n",
    "  disp = sns.heatmap(cm, annot=lable_list, fmt='', cmap='Blues')  # displays the results of the actual values \n",
    "                                                                 #  classified wrongly and correctly.                       \n",
    "  disp.plot()  # displaying data in plot\n",
    "  plt.title(title, fontsize=25)  # adding a title to plot\n",
    "  plt.ylabel('True label', fontsize=20)  # adding a y axis to the plot.\n",
    "  plt.xlabel('Predicted label' +\"\\n\\nScores\\n\" +total_score, fontsize=20)  # adding a x axis to the plot\n",
    "  plt.show()  # showing the plot\n",
    "    \n",
    "  result_list = train_accuracy,test_accuracy , precision, recall, f1_score  # returning the results\n",
    "  # plot single tree\n",
    "  plot_tree(model)\n",
    "  plt.show()\n",
    "  \n",
    "\n",
    "  return result_list  # returns the results from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb6aed2",
   "metadata": {},
   "source": [
    "# Feature Scaling and testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6542b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurescaling(Scaler, X_train, X_test, y_train, y_test, Modelandprams ,Modelname):\n",
    "    \n",
    "    sucidedataframe.info()  # checking Basic information on the dataframe being procesed.\n",
    "    \n",
    "    sc = Scaler # creating an instance of the object.\n",
    "    \n",
    "    print(\"Before scaling:\\nX_test \", X_test,\"\\n\\nX_train \", X_train)  # printing the sets before feature scaling.\n",
    "\n",
    "    X_train[:, 55:] = sc.fit_transform(X_train[:, 55:])  # Scaling x_train\n",
    "    X_test[:,55:] = sc.transform(X_test[:, 55:])  # Scaling y_train\n",
    "    \n",
    "    print(\"After scaling:\\nX_test \", X_test,\"\\n\\nX_train \", X_train)  # printing the sets after feature scaling.\n",
    "    print(\"\\nThe result of the model\")  \n",
    "    Classifier_function(Modelandprams , X_train, y_train,X_test, y_test, Modelname)  \n",
    "    # Performs traing, testing prediction.\n",
    "    # performs precision, recall, f1-score and support prediction\n",
    "    # plots a confusion matrix\n",
    "    # returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabca637",
   "metadata": {},
   "source": [
    "# Function to preform Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef53cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_search_fun(X, y):  # Function takes in X and y values hyperparameters.to perform grid search.\n",
    "    \n",
    "    # Grid Search\n",
    "    C_list = [0.25, 0.5, 0.75, 1]  # how much to avoid misclassifying each training example\n",
    "    kernel_list = ['linear', 'rbf']  # Kernel type\n",
    "    gamma_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # how far the influence of a single training example reaches\n",
    "\n",
    "    para_grid = dict(C=C_list,kernel=kernel_list, gamma=gamma_list)  # adding the above lists in a dictinary.\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    model = SVC()  # creating an instance of the object.\n",
    "    parameters = [para_grid]  # hyper parameters for the grid search\n",
    "    grid_search = GridSearchCV(estimator = model,  # model\n",
    "                           param_grid = parameters,  # hyper paramaters \n",
    "                           scoring = 'accuracy',  # score measurement\n",
    "                           cv = 5, # number of cross validations \n",
    "                           n_jobs = -1, return_train_score=False)  # selecting all possible paramaters to go \n",
    "    # through to get the best model possible # train score is false as it can be computationaly expensive. without \n",
    "    # storing the traning score the grd search is fater\n",
    "    grid_search.fit(X, y)  # applying the search on our model.\n",
    "    #print(pd.DataFrame(grid_search.cv_results_)[[\"mean_test_score\",\"params\"]])\n",
    "    print(pd.DataFrame(grid_search.cv_results_)) # to print the whole result\n",
    "\n",
    "    best_accuracy = grid_search.best_score_  # the best accuracy \n",
    "    best_parameters = grid_search.best_params_  # the best paramaters that gave the best accurecy\n",
    "    print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))  # printing best accuracy\n",
    "    print(\"Best Parameters:\", best_parameters)  # printing the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3631c12",
   "metadata": {},
   "source": [
    "# sub function to preform Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f39d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_search_fun(typeofmodelandprams, dict_prams,crossval, X, y):  # Function takes in the model type, number \n",
    "                                                             # of crossvalidation sand X and y values as hyperparameters.\n",
    "\n",
    "    model = typeofmodelandprams  # creating an instance of the object.\n",
    "    parameters = [dict_prams]  # hyper parameters for the random search \n",
    "    rand_search = RandomizedSearchCV(\n",
    "                           model,\n",
    "        #estimator = model,  # model\n",
    "                           #param_distributions = parameters,  # hyper paramaters \n",
    "                           parameters,\n",
    "                           scoring = 'accuracy',  # score measurement\n",
    "                           cv = crossval, # number of cross validations \n",
    "                           n_jobs = -1, # selecting all possible paramaters to go through to get the best model possible \n",
    "                           return_train_score=False, # train score is false as it can be computationaly expensive. \n",
    "                                                      # without storing the traning score the grd search is fater\n",
    "                           n_iter=10,  # setting the number of iterations\n",
    "                           random_state=5)  \n",
    "    rand_search.fit(X, y)  # applying the search on our model.\n",
    "    #print(pd.DataFrame(rand_search.cv_results_)[[\"mean_test_score\",\"params\"]])\n",
    "    print(pd.DataFrame(rand_search.cv_results_)) # to print the whole result\n",
    "\n",
    "    best_accuracy = rand_search.best_score_  # the best accuracy \n",
    "    best_parameters = rand_search.best_params_  # the best paramaters that gave the best accurecy\n",
    "    print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))  # printing best accuracy\n",
    "    print(\"Best Parameters:\", best_parameters)  # printing the best parameters\n",
    "\n",
    "\n",
    "\n",
    "    return rand_search  # return random search value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ea2bf",
   "metadata": {},
   "source": [
    "# Performs full Random Search on SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3dd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_full_rand_search(X, y):  # performs randomized search\n",
    "\n",
    "    # Random search \n",
    "\n",
    "    C_list = [0.25, 0.5, 0.75, 1]  # how much to avoid misclassifying each training example.\n",
    "    kernel_list = ['linear']  # Kernel type\n",
    "    gamma_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # how far the influence of a single training example reaches\n",
    "    para_rand = dict(C=C_list,kernel=kernel_list, gamma=gamma_list)  # adding the above lists in a dictinory.\n",
    "    \n",
    "\n",
    "    rand_search = rand_search_fun(SVC(), para_rand, 5, X, y)  # Using the ranmomized search gunction with\n",
    "    \n",
    "    C_list = [0.25, 0.5, 0.75, 1]  # list from 1 - 31 (will be used as KNN's)\n",
    "    kernel_list = ['rbf']   \n",
    "    gamma_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    para_rand = dict(C=C_list,kernel=kernel_list, gamma=gamma_list)  # adding the above lists in a dictinory.\n",
    "                                                                                          # arguments given \n",
    "    rand_search = rand_search_fun(SVC(), para_rand, 5, X, y)  # Using the grid search gunction with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb617071",
   "metadata": {},
   "source": [
    "## k-fold cross-validation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd9b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_plot_Values(X,y):  # Values required for plotting\n",
    "    ave_scores = []  # Creating a empty list\n",
    "    C_list = [0.25, 0.5, 0.75, 1]  # how much to avoid misclassifying each training example\n",
    "    for c in C_list:  # looping through each value in the C_list variable\n",
    "        model = SVC(C = c)   # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                                               # it to the ave_scores list.\n",
    "    print(\"Average C scores \", ave_scores)  # printing the average score of the model\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Best C Selection\",fontsize=18)  # Displays plot title\n",
    "    plt.plot(C_list, ave_scores)  # Displays description of the plots x and y labels.\n",
    "    plt.xlabel(\"C values\")  # Displays the x axis for the plot\n",
    "    plt.ylabel(\"Average CV model accuracy\")  # Displays the y axis for the plot\n",
    "    plt.legend([\"C\"], loc=\"lower right\")  # adds a legend to the plot.\n",
    "    plt.grid()  # adds a gird to the plot\n",
    "\n",
    "    \n",
    "    ave_scores = []  # Creating a empty list\n",
    "    gamma_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]  # how far the influence of a single training example reaches\n",
    "    for g in gamma_list:  # looping through each value in the C_list variable\n",
    "        model = SVC(gamma = g)   # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                    # it to the ave_scores list.\n",
    "    print(\"Average gamma scores \", ave_scores)  # printing the average score of the model\n",
    "    plt.figure()\n",
    "    plt.title(\"Best gamma Selection\",fontsize=18)  # Displays plot title\n",
    "    plt.plot(gamma_list, ave_scores)  # Displays description of the plots x and y labels.\n",
    "    plt.xlabel(\"gamma values\")  # Displays the x axis for the plot\n",
    "    plt.ylabel(\"Average CV model accuracy\")  # Displays the y axis for the plot\n",
    "    plt.legend([\"gamma\"], loc=\"lower right\")  # adds a legend to the plot.\n",
    "    plt.grid()  # adds a gird to the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5389e",
   "metadata": {},
   "source": [
    "## k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2694a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalscore(model, X, y, cv_val):  # function to perform cross validation with model X, y and cv_val as parameters\n",
    "    score = cross_val_score(estimator = model, X = X, y = y, cv = cv_val)   # performs different tests to get best accurecy.\n",
    "    print(\"Accuracy: {:.2f} %\".format(score.mean()*100))  # accuracy printed.\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(score.std()*100))  # standard deveation printed (std -avarage or std+ avarage )\n",
    "    return (\"Accuracy: {:.2f} %\".format(score.mean()*100))  # Accuracy is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d7ee1",
   "metadata": {},
   "source": [
    "# X and y un_edited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d43b7",
   "metadata": {},
   "source": [
    "## Plotting the SVM with a average Cross-val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_plot_Values(X,y)  # PLots SVM Cross Val score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7709ecd",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b90cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalscore(SVC(), X, y, 10)  # Performs cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7aa13",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b01ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Grid_search_fun(X, y)  # performs grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685b6c3",
   "metadata": {},
   "source": [
    "## Random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f685d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SVM_full_rand_search(X, y)  # performs random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff361c84",
   "metadata": {},
   "source": [
    "## Checking predicted/actual results\n",
    "## Checking testing and traning scores\n",
    "## Checking Actual values classified correctly and wrongly.\n",
    "## Checking accuracy, precision, recall and f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1186fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Classifier_function(SVC(), X_train, y_train,X_test, y_test, \"SVM Model\")  \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ded6a1",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results after feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057420aa",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(StandardScaler(), X_train, X_test, y_train, y_test,\\\n",
    "               SVM() ,\"SVM Model\")\n",
    "# performs standard scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50435668",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c0d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(MinMaxScaler(), X_train, X_test, y_train, y_test,\\\n",
    "               SVM() ,\"SVM Model\")\n",
    "# performs Minmax scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24856ddc",
   "metadata": {},
   "source": [
    "# RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524946ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(RobustScaler(), X_train, X_test, y_train, y_test,\\\n",
    "               SVM() ,\"SVM Model\")\n",
    "# performs robust scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47df896",
   "metadata": {},
   "source": [
    "# Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(Normalizer(), X_train, X_test, y_train, y_test,\\\n",
    "               SVM() ,\"SVM Model\")\n",
    "\n",
    "# Performs normalization scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7a010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e98a8b",
   "metadata": {},
   "source": [
    "# Over Sampled X and y values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307f100",
   "metadata": {},
   "source": [
    "## Plotting the SVM with a average Cross-val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_plot_Values(X_over,y_over)  # PLots SVM Cross Val score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277dbc2d",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalscore(SVC(), X_over, y_over, 10)  # Performs cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d5cf26",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e432a2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Grid_search_fun(X_over, y_over)  # performs grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53aefdd",
   "metadata": {},
   "source": [
    "## Random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df54186",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SVM_full_rand_search(X_over, y_over)  # performs random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e44df",
   "metadata": {},
   "source": [
    "## Checking predicted/actual results\n",
    "## Checking testing and traning scores\n",
    "## Checking Actual values classified correctly and wrongly.\n",
    "## Checking accuracy, precision, recall and f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34942679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Classifier_function(SVC(), X_train_over, y_train_over,X_test_over, y_test_over, \"SVM Model\")  \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb728e",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results after feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58e0c6",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(StandardScaler(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               SVM() ,\"SVM Model\")\n",
    "# performs standard scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b5ad4",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e70f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(MinMaxScaler(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               SVM() ,\"SVM Model\")\n",
    "# performs Minmax scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647b577",
   "metadata": {},
   "source": [
    "# RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(RobustScaler(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               SVM() ,\"SVM Model\")\n",
    "# performs robust scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d9b03",
   "metadata": {},
   "source": [
    "# Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(Normalizer(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               SVM() ,\"SVM Model\")\n",
    "\n",
    "# Performs normalization scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e4720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "929e2863",
   "metadata": {},
   "source": [
    "#  X and y obtained by using Corelation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc31b8",
   "metadata": {},
   "source": [
    "## Plotting the SVM with a average Cross-val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_plot_Values(X2,y)  # PLots SVM Cross Val score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7709ecd",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b90cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalscore(SVC(), X2, y, 10)  # Performs cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7aa13",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b01ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Grid_search_fun(X2, y)  # performs grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685b6c3",
   "metadata": {},
   "source": [
    "## Random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f685d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SVM_full_rand_search(X2, y)  # performs random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff361c84",
   "metadata": {},
   "source": [
    "## Checking predicted/actual results\n",
    "## Checking testing and traning scores\n",
    "## Checking Actual values classified correctly and wrongly.\n",
    "## Checking accuracy, precision, recall and f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1186fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Classifier_function(SVC(), X2_train, y2_train,X2_test, y2_test, \"SVM Model\")  \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
