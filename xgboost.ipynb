{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3fe8957",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860646ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run X_and_y_selection.ipynb  #  importing a .ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f10c4",
   "metadata": {},
   "source": [
    "# Function to evaluate different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier_function(model, X_train, y_train,X_test,y_test, title):  # function takes the name of the \n",
    "                                                               # model used, the x and y traning and testing sets.\n",
    "  model.fit(X_train, y_train)  # Building the k-nearest neighbors classification model.\n",
    "\n",
    "  y_test_p = model.predict(X_test)  # Predicted results.\n",
    "  print(\"  results\\npred-Actual\")  # printing predicted and real values.\n",
    "  print(np.concatenate((y_test_p.reshape(len(y_test_p),1),y_test.reshape(len(y_test),1)),1))  # Predicted results and \n",
    "                                                                                       #  real results in a np array.\n",
    "   \n",
    "\n",
    "  train_accuracy = round(model.score(X_train,y_train),2) * 100  # Getting traing accuracy multipling it by 100 after \n",
    "                                                                # rounding it by 2 to get a score between 0 to 100\n",
    "  test_accuracy = round(model.score(X_test,y_test),2) * 100  # Getting testing accuracy multipling it by 100 after \n",
    "                                                                # rounding it by 2 to get a score between 0 to 100\n",
    "\n",
    "  print(\"Model train accuracy: \", train_accuracy, \"%\")  # printing the model accurcy. \n",
    "  print(\"Model test accuracy: \", test_accuracy, \"%\")  # printing the model accurcy. \n",
    "\n",
    "\n",
    "  print(\"\\n\\n\")  # printing a new line.\n",
    "  # getting Accuracy or recall or precision or specificity\n",
    "  y_test_pred = model.predict(X_test)  # predicted results\n",
    "  \n",
    "  cReport = classification_report(y_test,y_test_pred)  # creating a Classification report\n",
    "  print(cReport)  # creating a Classification report\n",
    "  \n",
    "  cm = confusion_matrix(y_test, y_test_p)  # creating the confusion matrix\n",
    "  cm2 = multilabel_confusion_matrix(y_test, y_test_pred)  # creating a mutable confusion matrix\n",
    "\n",
    "\n",
    "  precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_test_pred)  # getting the precision,\n",
    "                                                                                          # recall and f1score for later use.\n",
    "  accuracy  = round(np.trace(cm) / float(np.sum(cm)), 2) * 100  # getting aaccuracy and multipling it by 100 after \n",
    "                                                                # rounding it by 2 to get a score between 0 to 100.\n",
    "  precision = round(np.mean(precision),2) * 100  # multipling precision variables mean by 100 after rounding it by\n",
    "                                                #  2 to get a score between 0 to 100.\n",
    "  recall = round(np.mean(recall),2) * 100  # multipling recall variables mean by 100 after rounding it by 2 to \n",
    "                                           # get a score between 0 to 100.\n",
    "  f1_score = round(np.mean(f1_score),2) * 100  # multipling f1_score variables mean by 100 after rounding \n",
    "                                               # it by 2 to get a score between 0 to 100.\n",
    "\n",
    "  lable_list = []  # creating a empty list\n",
    "\n",
    "  for i in range(len(cm)):  # looping in the range of the length of the confusion matrix.\n",
    "    for j in range(len(cm)):  # looping in the range of the length of the confusion matrix.\n",
    "        if j == i:  # if the value of j is equal to the value of i.\n",
    "            # the below code appends the Actual Values Classified correctly to the variable lable_list.\n",
    "            lable_list.append(\"Actual \"+ str(i) +\"\\n\" + \"calssified as \"+ str(j) +\"\\n\" + str(cm[i][j]) + \"\\n\"+ \\\n",
    "                              str(round(cm[i][j]/np.sum(cm),2)) + \" %\")\n",
    "\n",
    "        else:   # otherwise\n",
    "            #  the below function appends the actual values classified wrongly to the variable lable list.\n",
    "            lable_list.append(\"Actual \"+ str(i) +\"\\n\" + \"calssified as \"+ str(j) + \"\\n\"  + str(cm[i][j]) + \"\\n\"+ \\\n",
    "                              str(round(cm[i][j]/np.sum(cm),2)) + \" %\")\n",
    "\n",
    "            \n",
    "  lable_list = np.asarray(lable_list).reshape(len(cm),len(cm))  # resahping the label list as a numpy array to be \n",
    "                                                                # used in plotting the confusion matrix\n",
    "  \n",
    "  #  the variable function will be will be used to display the results of the evaluation to the confusion matrix.\n",
    "  total_score = (\"Accuracy:   \" + str(accuracy) +\" %\" + \"\\nPrecison:    \"  + str(precision)  +\" %\" + \"\\nRecall:        \" +\\\n",
    "                 str(recall)  +\" %\" + \"\\nF1 score:    \"  + str(f1_score) +\" %\") \n",
    "\n",
    "\n",
    "  # Below is the code used to plot the confusion matrix.\n",
    "  plt.figure(figsize = (12,9))  # sets the size of the matrix\n",
    "  disp = sns.heatmap(cm, annot=lable_list, fmt='', cmap='Blues')  # displays the results of the actual values \n",
    "                                                                 #  classified wrongly and correctly.                       \n",
    "  disp.plot()  # displaying data in plot\n",
    "  plt.title(title, fontsize=25)  # adding a title to plot\n",
    "  plt.ylabel('True label', fontsize=20)  # adding a y axis to the plot.\n",
    "  plt.xlabel('Predicted label' +\"\\n\\nScores\\n\" +total_score, fontsize=20)  # adding a x axis to the plot\n",
    "  plt.show()  # showing the plot\n",
    "    \n",
    "  result_list = train_accuracy,test_accuracy , precision, recall, f1_score  # returning the results\n",
    "\n",
    "  \n",
    "\n",
    "  return result_list  # returns the results from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd108d77",
   "metadata": {},
   "source": [
    "# Feature Scaling and testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6542b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurescaling(Scaler, X_train, X_test, y_train, y_test, Modelandprams ,Modelname):\n",
    "    \n",
    "    sucidedataframe.info()  # checking Basic information on the dataframe being procesed.\n",
    "    \n",
    "    sc = Scaler # creating an instance of the object.\n",
    "    \n",
    "    print(\"Before scaling:\\nX_test \", X_test,\"\\n\\nX_train \", X_train)  # printing the sets before feature scaling.\n",
    "\n",
    "    X_train[:, 55:] = sc.fit_transform(X_train[:, 55:])  # Scaling x_train\n",
    "    X_test[:,55:] = sc.transform(X_test[:, 55:])  # Scaling y_train\n",
    "    \n",
    "    print(\"After scaling:\\nX_test \", X_test,\"\\n\\nX_train \", X_train)  # printing the sets after feature scaling.\n",
    "    print(\"\\nThe result of the model\")  \n",
    "    Classifier_function(Modelandprams , X_train, y_train,X_test, y_test, Modelname)  \n",
    "    # Performs traing, testing prediction.\n",
    "    # performs precision, recall, f1-score and support prediction\n",
    "    # plots a confusion matrix\n",
    "    # returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabca637",
   "metadata": {},
   "source": [
    "# Function to preform Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef53cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_search_fun(X, y):  # Function takes in X and y values hyperparameters.to perform grid search.\n",
    "\n",
    "    # Grid Search\n",
    "    max_depth_list =  [2, 3, 4, 5, 6, 8, 10, 11, 12]  # the depth the tree is allowed to grow.\n",
    "    # used for regularization subsample, colsample_bylevel and colsample_bytree. If they are reduced more \n",
    "    # regularization effect can be achieved. Increasing them over fits the model.\n",
    "    subsample_list = [0.1, 0.5, 0.25, 0.75, 0.99]  # how much percentage of data is taken.\n",
    "    #colsample_bylevel_list = np.round(np.arange(0.1,1.0,0.01)) # by levels considring data percentages of features.\n",
    "    colsample_bytree_list = [0.1, 0.5, 0.25, 0.75, 0.99]  # the fraction of columns to be randomly samples for each tree\n",
    "    min_child_weight_list = [1,3,5,7] # stops trying to split once the sample size in a node goes \n",
    "                                                 # below a given threshold\n",
    "    #  increasing  reg_alpha or reg_lambda regulizes the model or generilizes the model                           \n",
    "    #reg_alpha_list = [0, 0.001, 0.005, 0.01, 0.05, 1]  # This is used to handle the regularization part of XGBoost.\n",
    "    #reg_lambda_list = [0, 0.001, 0.005, 0.01, 0.05, 1]  # It can be used in case of very high dimensionality \n",
    "                                               # so that the algorithm runs faster when implemented.\n",
    "    n_estimators_list =  [10,50,100,200] # gradiant booting algorythm each tree is bosting the next one.\n",
    "    learning_rate_list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]  # takes prcentage of residules and adds it back \n",
    "                                                # to the previous boosting tree.\n",
    "                                 \n",
    "\n",
    "    para_grid = dict(max_depth=max_depth_list, subsample=subsample_list,\\\n",
    "                   colsample_bytree=colsample_bytree_list, min_child_weight = min_child_weight_list, \\\n",
    "                    n_estimators = n_estimators_list,\\\n",
    "                   learning_rate = learning_rate_list )  # adding the above lists in a dictinary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = XGBClassifier(verbosity = 0)  # creating an instance of the object.\n",
    "    parameters = [para_grid]  # hyper parameters for the grid search\n",
    "    grid_search = GridSearchCV(estimator = model,  # model\n",
    "                           param_grid = parameters,  # hyper paramaters \n",
    "                           scoring = 'accuracy',  # score measurement\n",
    "                           cv = 5, # number of cross validations \n",
    "                           n_jobs = -1, return_train_score=False)  # selecting all possible paramaters to go \n",
    "    # through to get the best model possible # train score is false as it can be computationaly expensive. without \n",
    "    # storing the traning score the grd search is fater\n",
    "    grid_search.fit(X, y)  # applying the search on our model.\n",
    "    #print(pd.DataFrame(grid_search.cv_results_)[[\"mean_test_score\",\"params\"]])\n",
    "    print(pd.DataFrame(grid_search.cv_results_)) # to print the whole result\n",
    "\n",
    "    best_accuracy = grid_search.best_score_  # the best accuracy \n",
    "    best_parameters = grid_search.best_params_  # the best paramaters that gave the best accurecy\n",
    "    print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))  # printing best accuracy\n",
    "    print(\"Best Parameters:\", best_parameters)  # printing the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3631c12",
   "metadata": {},
   "source": [
    "# sub function to preform Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f39d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_search_fun(typeofmodelandprams, dict_prams,crossval, X, y):  # Function takes in the model type, number \n",
    "                                                             # of crossvalidation sand X and y values as hyperparameters.\n",
    "\n",
    "    model = typeofmodelandprams  # creating an instance of the object.\n",
    "    parameters = [dict_prams]  # hyper parameters for the random search \n",
    "    rand_search = RandomizedSearchCV(\n",
    "                           model,\n",
    "        #estimator = model,  # model\n",
    "                           #param_distributions = parameters,  # hyper paramaters \n",
    "                           parameters,\n",
    "                           scoring = 'accuracy',  # score measurement\n",
    "                           cv = crossval, # number of cross validations \n",
    "                           n_jobs = -1, # selecting all possible paramaters to go through to get the best model possible \n",
    "                           return_train_score=False, # train score is false as it can be computationaly expensive. \n",
    "                                                      # without storing the traning score the grd search is fater\n",
    "                           n_iter=10,  # setting the number of iterations\n",
    "                           random_state=5)  \n",
    "    rand_search.fit(X, y)  # applying the search on our model.\n",
    "    #print(pd.DataFrame(rand_search.cv_results_)[[\"mean_test_score\",\"params\"]])\n",
    "    print(pd.DataFrame(rand_search.cv_results_)) # to print the whole result\n",
    "\n",
    "    best_accuracy = rand_search.best_score_  # the best accuracy \n",
    "    best_parameters = rand_search.best_params_  # the best paramaters that gave the best accurecy\n",
    "    print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))  # printing best accuracy\n",
    "    print(\"Best Parameters:\", best_parameters)  # printing the best parameters\n",
    "\n",
    "\n",
    "\n",
    "    return rand_search  # return random search value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ea2bf",
   "metadata": {},
   "source": [
    "# Performs full Random Search on xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3dd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost_full_rand_search(X, y):  # performs randomized search\n",
    "\n",
    "    # Random search \n",
    "\n",
    "    max_depth_list =  [2, 3, 4, 5, 6, 8, 10, 11, 12]  # the depth the tree is allowed to grow.\n",
    "    min_child_weight_list = [1,3,5,7] # stops trying to split once the sample size in a node goes \n",
    "                                                 # below a given threshold\n",
    "    # increasing  reg_alpha or reg_lambda regulizes the model or generilizes the model                           \n",
    "    # reg_alpha_list = [0, 0.001, 0.005, 0.01, 0.05, 1]  # This is used to handle the regularization part of XGBoost.\n",
    "    # reg_lambda_list = [0, 0.001, 0.005, 0.01, 0.05, 1]  # It can be used in case of very high dimensionality \n",
    "                                               # so that the algorithm runs faster when implemented.\n",
    "    n_estimators_list =  [10,50,100,200] # gradiant booting algorythm each tree is bosting the next one.\n",
    "    learning_rate_list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]  # takes prcentage of residules and adds it back \n",
    "                                                # to the previous boosting tree.\n",
    "                                 \n",
    "\n",
    "    para_rand = dict(max_depth=max_depth_list,min_child_weight = min_child_weight_list, \\\n",
    "                    n_estimators = n_estimators_list,\\\n",
    "                   learning_rate = learning_rate_list )  # adding the above lists in a dictinary.\n",
    "\n",
    "\n",
    "    rand_search = rand_search_fun(XGBClassifier(verbosity = 0), para_rand, 5, X, y)  # Using the ranmomized search gunction with\n",
    "    \n",
    "    \n",
    "    max_depth_list =  [2, 3, 4, 5, 6, 8, 10, 11, 12]  # the depth the tree is allowed to grow.\n",
    "    # used for regularization subsample, colsample_bylevel and colsample_bytree. If they are reduced more \n",
    "    # regularization effect can be achieved. Increasing them over fits the model.\n",
    "    subsample_list = [0.1, 0.5, 0.25, 0.75, 0.99]  # how much percentage of data is taken.\n",
    "    # colsample_bylevel_list = np.round(np.arange(0.1,1.0,0.01)) # by levels considring data percentages of features.\n",
    "    colsample_bytree_list = [0.1, 0.5, 0.25, 0.75, 0.99]\n",
    "    # the fraction of columns to be randomly samples for each tree\n",
    "\n",
    "                                 \n",
    "\n",
    "    para_rand = dict(max_depth=max_depth_list, subsample=subsample_list, \\\n",
    "                   colsample_bytree=colsample_bytree_list)  # adding the above lists in a dictinary.\n",
    "\n",
    "\n",
    "    rand_search = rand_search_fun(XGBClassifier(verbosity = 0), para_rand, 5, X, y)  # Using the ranmomized search gunction with\n",
    "    \n",
    "    \n",
    "    max_depth_list =  [2, 3, 4, 5, 6, 8, 10, 11, 12]  # the depth the tree is allowed to grow.\n",
    "    # used for regularization subsample, colsample_bylevel and colsample_bytree. If they are reduced more \n",
    "    # regularization effect can be achieved. Increasing them over fits the model.\n",
    "    subsample_list = [0.1, 0.5, 0.25, 0.75, 0.99]  # how much percentage of data is taken.\n",
    "    # colsample_bylevel_list = np.round(np.arange(0.1,1.0,0.01)) # by levels considring data percentages of features.\n",
    "    colsample_bytree_list = [0.1, 0.5, 0.25, 0.75, 0.99]\n",
    "    # the fraction of columns to be randomly samples for each tree\n",
    "    min_child_weight_list = [1,3,5,7] # stops trying to split once the sample size in a node goes \n",
    "                                                 # below a given threshold\n",
    "    # increasing  reg_alpha or reg_lambda regulizes the model or generilizes the model                           \n",
    "    # reg_alpha_list = [0, 0.001, 0.005, 0.01, 0.05, 1]  # This is used to handle the regularization part of XGBoost.\n",
    "    # reg_lambda_list = [0, 0.001, 0.005, 0.01, 0.05, 1]  # It can be used in case of very high dimensionality \n",
    "                                               # so that the algorithm runs faster when implemented.\n",
    "    n_estimators_list =  [10,50,100,200] # gradiant booting algorythm each tree is bosting the next one.\n",
    "    learning_rate_list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]  # takes prcentage of residules and adds it back \n",
    "                                                # to the previous boosting tree.\n",
    "                                 \n",
    "\n",
    "    para_rand = dict(max_depth=max_depth_list, subsample=subsample_list, \\\n",
    "                   colsample_bytree=colsample_bytree_list, min_child_weight = min_child_weight_list, \\\n",
    "                    n_estimators = n_estimators_list,\\\n",
    "                   learning_rate = learning_rate_list )  # adding the above lists in a dictinary.\n",
    "\n",
    "\n",
    "    rand_search = rand_search_fun(XGBClassifier(verbosity = 0), para_rand, 5, X, y)  # Using the ranmomized search gunction with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb617071",
   "metadata": {},
   "source": [
    "## k-fold cross-validation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb60f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_subplot(list_for_plot, ave_scores, hyper_pram_to_use_name):\n",
    "    \n",
    "    print(\"Average \" + hyper_pram_to_use_name + \" scores\", ave_scores)  # printing the average score of the model\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Best \" + hyper_pram_to_use_name + \" Selection\",fontsize=18)  # Displays plot title\n",
    "    plt.plot(list_for_plot, ave_scores)  # Displays description of the plots x and y labels.\n",
    "    plt.xlabel(hyper_pram_to_use_name +\" values\")  # Displays the x axis for the plot\n",
    "    plt.ylabel(\"Average CV model accuracy\")  # Displays the y axis for the plot\n",
    "    plt.legend([hyper_pram_to_use_name], loc=\"lower right\")  # adds a legend to the plot.\n",
    "    plt.grid()  # adds a gird to the plot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd9b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_plot_Values(X,y):  # Values required for plotting\n",
    "    \n",
    "    learning_rate_list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]  # takes prcentage of residules and adds it back \n",
    "                                               # to the previous boosting tree.\n",
    "    ave_scores = []  # Creating a empty list\n",
    "    max_depth_list =  [2, 3, 4, 5, 6, 8, 10, 11, 12]  # the depth the tree is allowed to grow.\n",
    "    # used for regularization subsample, colsample_bylevel and colsample_bytree. If they are reduced more \n",
    "    # regularization effect can be achieved. Increasing them over fits the model.\n",
    "    for l in max_depth_list:  # looping through each value in the list variable\n",
    "        model =  XGBClassifier(verbosity = 0, max_depth=l)  # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                                               # it to the ave_scores list.\n",
    "    xgboost_subplot(max_depth_list, ave_scores ,\"max_depth\")\n",
    "    \n",
    "\n",
    "    ave_scores = []  # Creating a empty list\n",
    "    subsample_list = [0.1, 0.5, 0.25, 0.75, 0.99]  # how much percentage of data is taken.\n",
    "    # colsample_bylevel_list = np.round(np.arange(0.1,1.0,0.01)) # by levels considring data percentages of features.\n",
    "    for l in subsample_list:  # looping through each value in the list variable\n",
    "        model =  XGBClassifier(verbosity = 0, subsample=l)  # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                                               # it to the ave_scores list.\n",
    "    xgboost_subplot(subsample_list, ave_scores, \"subsample\")\n",
    "    \n",
    "    \n",
    "\n",
    "    ave_scores = []  # Creating a empty list\n",
    "    colsample_bytree_list = [0.1, 0.5, 0.25, 0.75, 0.99]   # the fraction of columns to be randomly samples for each tree\n",
    "    # colsample_bylevel_list = np.round(np.arange(0.1,1.0,0.01)) # by levels considring data percentages of features.\n",
    "    for l in colsample_bytree_list:  # looping through each value in the list variable\n",
    "        model =  XGBClassifier(verbosity = 0, colsample_bytree=l)  # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                                               # it to the ave_scores list.\n",
    "    xgboost_subplot(colsample_bytree_list, ave_scores, \"colsample\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ave_scores = []  # Creating a empty list   \n",
    "    min_child_weight_list = [1,3,5,7] # stops trying to split once the sample size in a node goes \n",
    "                                      # below a given threshold\n",
    "    for l in min_child_weight_list:  # looping through each value in the list variable\n",
    "        model =  XGBClassifier(verbosity = 0, min_child_weight=l)  # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                                               # it to the ave_scores list.   \n",
    "    xgboost_subplot(min_child_weight_list, ave_scores, \"min_child_weight\")\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ave_scores = []  # Creating a empty list   \n",
    "    n_estimators_list =  [10,50,100,200] # gradiant booting algorythm each tree is bosting the next one.\n",
    "    for l in min_child_weight_list:  # looping through each value in the list variable\n",
    "        model =  XGBClassifier(verbosity = 0, n_estimators=l)  # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                                               # it to the ave_scores list.   \n",
    "    xgboost_subplot(n_estimators_list, ave_scores, \"n_estimators_list\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ave_scores = []  # Creating a empty list   \n",
    "    learning_rate_list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]  # takes prcentage of residules and adds it back \n",
    "                                                            # to the previous boosting tree.\n",
    "    for l in learning_rate_list:  # looping through each value in the list variable\n",
    "        model =  XGBClassifier(verbosity = 0, learning_rate=l)  # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                                               # it to the ave_scores list.       \n",
    "    \n",
    "    xgboost_subplot(learning_rate_list, ave_scores, \"learning_rate\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ave_scores = []  # Creating a empty list   \n",
    "    reg_alpha_list = [0, 0.001, 0.005, 0.01, 0.05, 1]  # This is used to handle the regularization part of XGBoost.\n",
    "    for l in reg_alpha_list:  # looping through each value in the list variable\n",
    "        model =  XGBClassifier(verbosity = 0, reg_alpha=l)  # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                                               # it to the ave_scores list.       \n",
    "    \n",
    "    xgboost_subplot(reg_alpha_list, ave_scores, \"reg_alpha\")    \n",
    "    \n",
    "\n",
    "    \n",
    "    ave_scores = []  # Creating a empty list   \n",
    "    reg_lambda_list = [0, 0.001, 0.005, 0.01, 0.05, 1]  # It can be used in case of very high dimensionality \n",
    "                                               # so that the algorithm runs faster when implemented.\n",
    "    for l in reg_lambda_list:  # looping through each value in the list variable\n",
    "        model =  XGBClassifier(verbosity = 0, reg_lambda=l)  # creating an instance of a class.\n",
    "        scores = cross_val_score(model,X,y,cv=5,scoring=\"accuracy\")  # Getting the results of the model.\n",
    "        ave_scores.append(round(scores.mean(),3))  # getting the average score from the model and appending \n",
    "                                                                               # it to the ave_scores list.       \n",
    "    \n",
    "    xgboost_subplot(reg_lambda_list, ave_scores, \"reg_lambda\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5389e",
   "metadata": {},
   "source": [
    "## k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2694a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalscore(model, X, y, cv_val):  # function to perform cross validation with model X, y and cv_val as parameters\n",
    "    score = cross_val_score(estimator = model, X = X, y = y, cv = cv_val)   # performs different tests to get best accurecy.\n",
    "    print(\"Accuracy: {:.2f} %\".format(score.mean()*100))  # accuracy printed.\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(score.std()*100))  # standard deveation printed (std -avarage or std+ avarage )\n",
    "    return (\"Accuracy: {:.2f} %\".format(score.mean()*100))  # Accuracy is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d7ee1",
   "metadata": {},
   "source": [
    "# X and y un_edited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d43b7",
   "metadata": {},
   "source": [
    "## Plotting the xgboost with a average Cross-val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_plot_Values(X,y)  # PLots xgboost Cross Val score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7709ecd",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b90cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalscore(XGBClassifier(verbosity = 0), X, y, 5)  # Performs cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7aa13",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b01ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Grid_search_fun(X, y)  # performs grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685b6c3",
   "metadata": {},
   "source": [
    "## Random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f685d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "XGBoost_full_rand_search(X, y)  # performs random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff361c84",
   "metadata": {},
   "source": [
    "## Checking predicted/actual results\n",
    "## Checking testing and traning scores\n",
    "## Checking Actual values classified correctly and wrongly.\n",
    "## Checking accuracy, precision, recall and f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1186fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Classifier_function(XGBClassifier(verbosity = 0), X_train, y_train,X_test, y_test, \"XGBoost\")  \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ded6a1",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results after feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fdc1c",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(StandardScaler(), X_train, `X_test, y_train, y_test,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "# performs standard scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971d3f5",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709facc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(MinMaxScaler(), X_train, X_test, y_train, y_test,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "# performs Minmax scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39262773",
   "metadata": {},
   "source": [
    "# RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(RobustScaler(), X_train, X_test, y_train, y_test,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "# performs robust scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb527b",
   "metadata": {},
   "source": [
    "# Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(Normalizer(), X_train, X_test, y_train, y_test,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "\n",
    "# Performs normalization scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6b6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68536cf4",
   "metadata": {},
   "source": [
    "# Over Sampled X and y values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1414a4",
   "metadata": {},
   "source": [
    "## Plotting the xgboost with a average Cross-val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_plot_Values(X_over,y_over)  # PLots xgboost Cross Val score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2338a9e",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalscore(XGBClassifier(verbosity = 0), X, y, 5)  # Performs cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1eb66",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503b5e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Grid_search_fun(X_over, y_over)  # performs grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f4963",
   "metadata": {},
   "source": [
    "## Random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc66f6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "XGBoost_full_rand_search(X_over, y_over)  # performs random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759efde",
   "metadata": {},
   "source": [
    "## Checking predicted/actual results\n",
    "## Checking testing and traning scores\n",
    "## Checking Actual values classified correctly and wrongly.\n",
    "## Checking accuracy, precision, recall and f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec29656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Classifier_function(XGBClassifier(verbosity = 0), X_train_over, y_train_over,X_test_over, y_test_over, \"XGBoost\")  \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914a89a",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results after feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b78d4",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(StandardScaler(), X_train_over, `X_test_over, y_train_over, y_test_over,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "# performs standard scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971d3f5",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709facc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(MinMaxScaler(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "# performs Minmax scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39262773",
   "metadata": {},
   "source": [
    "# RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(RobustScaler(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "# performs robust scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb527b",
   "metadata": {},
   "source": [
    "# Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(Normalizer(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "\n",
    "# Performs normalization scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be792c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "328d7ee1",
   "metadata": {},
   "source": [
    "# Over Sampled X and y values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d43b7",
   "metadata": {},
   "source": [
    "## Plotting the xgboost with a average Cross-val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_plot_Values(X_over,y_over)  # PLots xgboost Cross Val score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7709ecd",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b90cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalscore(XGBClassifier(verbosity = 0), X, y, 5)  # Performs cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7aa13",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b01ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Grid_search_fun(X_over, y_over)  # performs grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685b6c3",
   "metadata": {},
   "source": [
    "## Random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f685d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "XGBoost_full_rand_search(X_over, y_over)  # performs random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff361c84",
   "metadata": {},
   "source": [
    "## Checking predicted/actual results\n",
    "## Checking testing and traning scores\n",
    "## Checking Actual values classified correctly and wrongly.\n",
    "## Checking accuracy, precision, recall and f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1186fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Classifier_function(XGBClassifier(verbosity = 0), X_train_over, y_train_over,X_test_over, y_test_over, \"XGBoost\")  \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ded6a1",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Results after feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fdc1c",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(StandardScaler(), X_train_over, `X_test_over, y_train_over, y_test_over,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "# performs standard scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971d3f5",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709facc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(MinMaxScaler(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "# performs Minmax scaling\n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39262773",
   "metadata": {},
   "source": [
    "# RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(RobustScaler(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "# performs robust scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb527b",
   "metadata": {},
   "source": [
    "# Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fb0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescaling(Normalizer(), X_train_over, X_test_over, y_train_over, y_test_over,\\\n",
    "               XGBClassifier(verbosity = 0) ,\"XGBoost\")\n",
    "\n",
    "# Performs normalization scaling \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d7ee1",
   "metadata": {},
   "source": [
    "#  X and y obtained by using Corelation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d43b7",
   "metadata": {},
   "source": [
    "## Plotting the xgboost with a average Cross-val score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_plot_Values(X,y)  # PLots xgboostKNN Cross Val score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7709ecd",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b90cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalscore(XGBClassifier(verbosity = 0), X, y, 5)  # Performs cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7aa13",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b01ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Grid_search_fun(X, y)  # performs grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685b6c3",
   "metadata": {},
   "source": [
    "## Random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f685d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "XGBoost_full_rand_search(X, y)  # performs random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff361c84",
   "metadata": {},
   "source": [
    "## Checking predicted/actual results\n",
    "## Checking testing and traning scores\n",
    "## Checking Actual values classified correctly and wrongly.\n",
    "## Checking accuracy, precision, recall and f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1186fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Classifier_function(XGBClassifier(), X_train, y_train,X_test, y_test, \"XGBoost\")  \n",
    "# Performs traing, testing prediction.\n",
    "# performs precision, recall, f1-score and support prediction\n",
    "# plots a confusion matrix\n",
    "# returns the traing, testing, precision, recall, f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2ad59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
